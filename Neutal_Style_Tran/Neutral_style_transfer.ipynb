{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d74514-284a-4131-bc61-460f677634bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select the Content Image\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tkinter import Tk, filedialog\n",
    "\n",
    "# Function to select an image file\n",
    "def select_image(title):\n",
    "    root = Tk()\n",
    "    root.withdraw()  # Hide the main window\n",
    "    file_path = filedialog.askopenfilename(title=title, filetypes=[(\"Image Files\", \"*.jpg;*.jpeg;*.png\")])\n",
    "    return file_path\n",
    "\n",
    "def load_image(img_path, max_size=400, shape=None):\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    if max(image.size) > max_size:\n",
    "        size = max_size\n",
    "    else:\n",
    "        size = max(image.size)\n",
    "    if shape is not None:\n",
    "        size = shape\n",
    "    in_transform = transforms.Compose([\n",
    "        transforms.Resize(size if isinstance(size, tuple) else (size, size), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "    image = in_transform(image)[:3, :, :].unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "def get_features(image, model, layers=None):\n",
    "    if layers is None:\n",
    "        layers = {'0': 'conv1_1', '5': 'conv2_1', '10': 'conv3_1', '19': 'conv4_1', '21': 'conv4_2', '28': 'conv5_1'}\n",
    "    features = {}\n",
    "    x = image\n",
    "    for name, layer in model._modules.items():\n",
    "        x = layer(x)\n",
    "        if name in layers:\n",
    "            features[layers[name]] = x\n",
    "    return features\n",
    "\n",
    "def gram_matrix(tensor):\n",
    "    _, d, h, w = tensor.size()\n",
    "    tensor = tensor.view(d, h * w)\n",
    "    gram = torch.mm(tensor, tensor.t())\n",
    "    return gram \n",
    "\n",
    "# Select images from file dialog\n",
    "print(\"Select the Content Image\")\n",
    "content_path = select_image(\"Select Content Image\")\n",
    "\n",
    "print(\"Select the Style Image\")\n",
    "style_path = select_image(\"Select Style Image\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features.to(device).eval()\n",
    "\n",
    "content = load_image(content_path).to(device)\n",
    "style = load_image(style_path, shape=(content.shape[-2], content.shape[-1])).to(device)\n",
    "\n",
    "content_features = get_features(content, vgg)\n",
    "style_features = get_features(style, vgg)\n",
    "style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
    "\n",
    "target = content.clone().requires_grad_(True).to(device)\n",
    "\n",
    "style_weights = {'conv1_1': 1.0, 'conv2_1': 0.75, 'conv3_1': 0.5, 'conv4_1': 0.25, 'conv5_1': 0.1}\n",
    "content_weight = 1e4\n",
    "style_weight = 1e2\n",
    "\n",
    "optimizer = optim.Adam([target], lr=0.003)\n",
    "\n",
    "print(\"Starting Neural Style Transfer...\")\n",
    "for i in range(300):\n",
    "    target_features = get_features(target, vgg)\n",
    "    content_loss = torch.nn.functional.mse_loss(target_features['conv4_2'], content_features['conv4_2'])\n",
    "    style_loss = 0\n",
    "    for layer in style_weights:\n",
    "        target_feature = target_features[layer]\n",
    "        target_gram = gram_matrix(target_feature)\n",
    "        _, d, h, w = target_feature.shape\n",
    "        style_gram = style_grams[layer]\n",
    "        layer_style_loss = torch.nn.functional.mse_loss(target_gram, style_gram)\n",
    "        style_loss += (style_weights[layer] * layer_style_loss) / (d * h * w)\n",
    "    \n",
    "    total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 50 == 0:  # Print progress every 50 iterations\n",
    "        print(f\"Iteration {i}/300, Total Loss: {total_loss.item()}\")\n",
    "\n",
    "# Convert output to an image\n",
    "target_image = target.clone().detach().cpu().squeeze(0)\n",
    "target_image = torch.clamp(target_image, 0, 1)\n",
    "target_image = transforms.ToPILImage()(target_image)\n",
    "\n",
    "# Save & display the final image\n",
    "target_image.save(\"output.jpg\")\n",
    "print(\"Image saved as output.jpg\")\n",
    "\n",
    "plt.imshow(target_image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdd9644-b3d6-411d-b1fd-c9345f282c81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
